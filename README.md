# gemma3.c

> ğŸš€ **Pure C inference engine for Google's Gemma 3 4B IT model**
>
> A fully working, dependencyâ€‘free implementation of a modern large language model, written in pure C.
> No Python, no PyTorch, no CUDA. Just you, your CPU, and a lot of floatingâ€‘point math.

---

## âœ¨ Highlights

* âš™ï¸ **100% Pure C (C11)** â€“ zero external dependencies
* ğŸ§  **Full Gemma 3 architecture** â€“ GQA, hybrid local/global attention, SwiGLU MLP
* ğŸ—ºï¸ **Memoryâ€‘mapped weights** â€“ efficient loading via `mmap` from BF16 SafeTensors
* ğŸ”¤ **Native SentencePiece tokenizer** â€“ protobuf parsing, 262K vocabulary
* ğŸŒŠ **Streaming output** â€“ tokenâ€‘byâ€‘token callbacks
* ğŸ’¬ **Interactive chat mode** â€“ with Gemma 3 chat templates
* ğŸ“¦ **Library + CLI** â€“ use it as a C library or as a standalone executable
* ğŸ§ **POSIXâ€‘first design** â€“ native on Linux and macOS
* ğŸªŸ **Windows via compatibility layers** â€“ WSL (recommended) or MinGW

---

## ğŸ“¸ What is this project?

`gemma3.c` is a **fromâ€‘scratch CPU inference engine** for the *Gemma 3 4B IT* model.
It demonstrates that modern LLMs can be run without frameworks, without Python, and without GPUs.

This is not a toy: it fully loads the official model, runs inference, streams tokens, and supports chat.

---

## ğŸš€ Quick Start

> âš ï¸ **Note on Windows**
> `gemma3.c` is a **POSIX-first project**. It runs natively on Linux and macOS.
> On Windows you must use **WSL** (recommended) or build with **MinGW** (with reduced features, no `mmap`).

### 1ï¸âƒ£ Download the model (recommended: Python script)

The fastest and safest way to get the Gemma 3 model is via the builtâ€‘in Python downloader:

```bash
python download_model.py --token YOUR_HF_TOKEN
```

Or set your token once:

```bash
export HF_TOKEN=your_token_here
python download_model.py
```

This will create the `./gemma-3-4b-it` directory with all required files.

---

### 2ï¸âƒ£ Build the project

```bash
make
```

---

### 3ï¸âƒ£ Run

```bash
# Run a single prompt
./gemma3 -m ./gemma-3-4b-it -p "Explain quantum computing in simple terms."

# Interactive chat
./gemma3 -m ./gemma-3-4b-it -i

# Custom system prompt
./gemma3 -m ./gemma-3-4b-it -i -s "You are a pirate. Respond in pirate speak."
```

---

## ğŸ› ï¸ Building

### Requirements

* C11 compiler (GCC / Clang)
* ~3â€“4 GB of free RAM

### Linux / macOS

```bash
make          # Optimized build (-O3)
make debug    # Debug symbols
make fast     # Aggressive CPU optimizations
make clean    # Cleanup
```

### ğŸªŸ Windows

Two options:

#### Option 1 â€” WSL (Recommended)

Install WSL and Ubuntu, then:

```bash
sudo apt update
sudo apt install build-essential
make
```

This gives you the exact same environment as Linux.

#### Option 2 â€” MinGW

```bash
gcc -O3 -std=c11 -o gemma3.exe *.c
```

Note: Windows builds use standard file IO instead of `mmap`.

---

## ğŸ“¥ Model Download (Recommended way: Python script)

The repository includes a **fully automated Python downloader** that:

* Handles HuggingFace authentication
* Downloads all model shards
* Resumes broken downloads
* Verifies integrity

### ğŸ”¥ Oneâ€‘command setup

```bash
python download_model.py --token YOUR_HF_TOKEN
```

Or set the token once:

```bash
export HF_TOKEN=your_token_here
python download_model.py
```

This is the **recommended method**.

---

### Manual alternatives

```bash
# huggingface-cli
pip install huggingface_hub
huggingface-cli download google/gemma-3-4b-it --local-dir ./gemma-3-4b-it

# or git-lfs
git lfs install
git clone https://huggingface.co/google/gemma-3-4b-it
```

The model directory must contain:

* `model*.safetensors`
* `tokenizer.model`

---

## ğŸ§ª Usage

### CLI Options

```
-m, --model <path>      Path to model directory (required)
-p, --prompt <text>     Input prompt
-i, --interactive       Interactive chat
-s, --system <text>     System prompt
-n, --max-tokens <n>    Max tokens (default 512)
-t, --temperature <f>   Temperature (default 0.7)
-k, --top-k <n>         Topâ€‘k sampling
--top-p <f>             Topâ€‘p sampling
-c, --context <n>       Context size
--seed <n>              RNG seed
-v, --verbose           Verbose output
```

---

## ğŸ“š Library API

```c
#include "gemma3.h"

gemma3_ctx *ctx = gemma3_load_dir("./gemma-3-4b-it");

gemma3_gen_params params = gemma3_default_params();
char *out = gemma3_generate(ctx, "Hello!", &params, NULL, NULL);
printf("%s\n", out);
free(out);

gemma3_free(ctx);
```

Streaming:

```c
int cb(int id, const char *tok, void *u) {
    printf("%s", tok);
    return 0;
}
```

---

## ğŸ§  Architecture

| Parameter       | Value              |
| --------------- | ------------------ |
| Vocabulary      | 262,208            |
| Hidden size     | 2,560              |
| Layers          | 34                 |
| Attention heads | 8                  |
| KV heads        | 4 (GQA)            |
| Context length  | 128K               |
| Sliding window  | 1,024              |
| Pattern         | 5 local : 1 global |

---

## ğŸ’¾ Memory

| Component           | Size       |
| ------------------- | ---------- |
| Weights (BF16 mmap) | ~8 GB disk |
| KV cache            | ~70 MB     |
| Activations         | ~100 MB    |
| **Total RAM**       | **~3 GB**  |

Lower memory:

```bash
./gemma3 -m ./gemma-3-4b-it -c 512 -p "Hello"
```

---

## âš¡ Performance (CPU)

* Prefill: 2â€“5 tok/s
* Generation: 1â€“3 tok/s

Optimizations:

```bash
make fast
```

---

## âš ï¸ Limitations

* Textâ€‘only
* CPU only
* No quantization (yet)

---

## ğŸ§© Project Layout

```
gemma3.c/
â”œâ”€â”€ gemma3.h
â”œâ”€â”€ gemma3.c
â”œâ”€â”€ gemma3_transformer.c
â”œâ”€â”€ gemma3_safetensors.c
â”œâ”€â”€ gemma3_tokenizer.c
â”œâ”€â”€ gemma3_kernels.c
â”œâ”€â”€ main.c
â”œâ”€â”€ download_model.py
â””â”€â”€ README.md
```

---

## ğŸªª License

MIT License.
Model weights are under Googleâ€™s Gemma license.

---

## ğŸ™Œ Credits

Inspired by:

* llama.cpp
* llama2.c
* flux2.c

---

If you ever wanted to see an LLM breathe in pure C, this is it.
